{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvJeCou4EHJeOWKM4uF0d6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tugas 01 : Crawling Berita"],"metadata":{"id":"-FH6Dz3KgbDb"}},{"cell_type":"markdown","source":["**Pengertian Crawling**\n","\n","\n","Crawling adalah proses otomatisasi yang dilakukan oleh program komputer untuk menjelajahi dan mengumpulkan data dari halaman-halaman web di internet. Proses ini sering kali dilakukan oleh bot yang dikenal sebagai web crawlers atau spiders. Web crawlers ini akan menelusuri (crawl) berbagai situs web, mengakses halaman-halaman yang ada, dan mengunduh atau mengekstraksi informasi yang dibutuhkan untuk kemudian disimpan atau diindeks dalam database."],"metadata":{"id":"q0XAJBRjeE0X"}},{"cell_type":"markdown","source":["**Fungsi Crawling**\n","\n","\n","\n","1.   Mengindeks Halaman Web:\n","\n","\n","*   Crawling adalah langkah pertama dalam proses pengindeksan halaman web oleh mesin pencari. Web crawler akan mengunjungi halaman-halaman web, membaca kontennya, dan menyimpannya ke dalam indeks mesin pencari. Ini memungkinkan mesin pencari untuk menemukan dan menampilkan halaman-halaman tersebut dalam hasil pencarian.\n","\n","\n","\n","2.   Pengumpulan Data:\n","\n","*   Crawling memungkinkan pengumpulan data dari berbagai situs web untuk keperluan tertentu, seperti analisis bisnis, riset pasar, atau pengembangan model kecerdasan buatan. Dengan crawling, data dari berbagai sumber bisa dikumpulkan secara otomatis tanpa harus melakukannya secara manual.\n","\n","\n","\n","3. Pemantauan Perubahan Konten:\n","\n","*   Dengan crawling, perubahan atau pembaruan pada suatu situs web dapat dipantau secara berkala. Ini berguna untuk aplikasi yang memerlukan informasi terbaru, seperti agregator berita, alat pemantau harga, atau layanan notifikasi.\n","\n","\n","\n","4. SEO (Search Engine Optimization):\n","\n","*   Bagi pengelola situs web, memahami bagaimana proses crawling bekerja dapat membantu dalam optimasi mesin pencari (SEO). Dengan memastikan bahwa halaman-halaman web mereka mudah di-crawl dan diindeks, mereka bisa meningkatkan kemungkinan situs web mereka muncul di hasil pencarian mesin pencari.\n"],"metadata":{"id":"0hhGpPZnegJY"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","# Fungsi untuk mengambil data dari halaman web Detik.com\n","def get_data(url, kategori):\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Request failed: {e}\")\n","        return\n","\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    articles = soup.find_all(\"article\", class_=\"list-content__item\")\n","\n","    for article in articles:\n","        if len(judul) >= 10:  # Hentikan jika sudah 10 berita\n","            return\n","\n","        try:\n","            link = article.find(\"a\")[\"href\"]\n","            article_response = requests.get(link)\n","            article_response.raise_for_status()\n","        except (requests.exceptions.RequestException, TypeError) as e:\n","            print(f\"Request for article failed: {e}\")\n","            continue\n","\n","        article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n","        title_element = article_soup.find(\"h1\", class_=\"detail__title\")\n","        title = title_element.text.strip() if title_element else \"Title Not Found\"\n","        date_element = article_soup.find(\"div\", class_=\"detail__date\")\n","        date = date_element.text.strip() if date_element else \"Date Not Found\"\n","        content_element = article_soup.find(\"div\", class_=\"detail__body-text\")\n","        content = content_element.text.strip() if content_element else \"Content Not Found\"\n","\n","        judul.append(title)\n","        tanggal.append(date)\n","        isi.append(content)\n","\n","        print(title)\n","        time.sleep(1)  # Menambahkan jeda waktu 1 detik antara permintaan artikel\n","\n","# Membuat list url dan kategori yang akan di-crawl\n","base_urls = [\"https://oto.detik.com/indeks\"]\n","categories = [\"Otomotif\"]\n","\n","# Inisialisasi list untuk menyimpan data\n","judul = []\n","tanggal = []\n","isi = []\n","\n","# Melakukan iterasi untuk setiap url dan kategori\n","for base_url, category in zip(base_urls, categories):\n","    for page in range(1, 4):  # Looping untuk beralih halaman\n","        if len(judul) >= 10:  # Hentikan jika sudah 10 berita\n","            break\n","\n","        url = f\"{base_url}/{page}\"\n","        get_data(url, category)\n","        time.sleep(2)  # Menambahkan jeda waktu 2 detik antara permintaan halaman\n","\n","# Membuat dataframe dari list data\n","df = pd.DataFrame({\n","    \"judul\": judul,\n","    \"tanggal\": tanggal,\n","    \"isi\": isi\n","})\n","\n","# Menyimpan dataframe ke file CSV\n","df.to_csv(\"Crawl-berita-Otomotif.csv\", index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oByQn8w6jIpB","outputId":"b56aedbf-3306-44a0-b2b2-f8c359eaa8c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Rayakan Ultah Ke-18, Komunitas Innova Buktikan Keiritan Mobil Hybrid\n"]}]},{"cell_type":"code","source":["df=pd.read_csv(\"Crawl-berita-Otomotif.csv\")\n","df"],"metadata":{"id":"KKywMWqhiSoc"},"execution_count":null,"outputs":[]}]}